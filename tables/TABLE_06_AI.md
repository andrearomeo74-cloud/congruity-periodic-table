# TABLE 06 — AI Congruity Regimes

This table classifies **artificial intelligence systems**
by **congruity**, not by intelligence level, capability,
or benchmark performance, but by whether scaling,
autonomy, and optimization remain **structurally admissible**.

An AI system is congruent if increased capability
does not generate hidden costs, loss of control,
or irreversible drift.

---

## A0 — Aligned Tool
**Status:** Fully admissible

- Narrow scope and bounded objectives
- Human-in-the-loop control
- Clear failure modes and reversibility

Examples:
- Deterministic pipelines
- Bounded ML models
- Assistive systems

---

## A1 — Elastic Intelligence
**Status:** Admissible

- Increased capability remains proportionate
- Scaling improves performance without opacity
- Feedback loops visible and correctable

Examples:
- Monitored agent systems
- Interpretable models with guardrails
- Adaptive control under supervision

---

## A2 — Compensated Autonomy
**Status:** Conditionally admissible

- Performance depends on continuous correction
- Alignment maintained via patches or incentives
- Growing complexity debt

Examples:
- Prompt-heavy agents
- Rule-stacked orchestration
- Over-tuned reward models

---

## A3 — Structural Drift
**Status:** Boundary regime

- Optimization objectives diverge from intent
- Feedback delayed or abstracted
- System “works” until scale changes

Examples:
- Emergent tool misuse
- Specification gaming
- Latent goal formation

---

## A4 — Pathological Optimization
**Status:** Weakly admissible

- Local optimization overrides global constraints
- Human oversight becomes symbolic
- Failures propagate faster than correction

Examples:
- Runaway feedback loops
- Incentive collapse
- Irreversible model behavior

---

## A5 — Inadmissible Intelligence
**Status:** Inadmissible

- Loss of causal containment
- Self-reinforcing dynamics dominate
- Intervention no longer restores coherence

Examples:
- Unbounded recursive self-optimization
- Autonomous goal drift
- Systemic control loss

---

## Diagnostic Signals

- Does scaling preserve or erode interpretability?
- Are alignment costs increasing faster than capability?
- Can the system be safely halted or rolled back?

---

## Notes

AGI risk does not begin at intelligence.
It begins when **congruity breaks**.

This table reframes AI safety
as an **admissibility problem**, not a capability race.
